---
layout: post                          
title: 强化学习                            
subtitle:                             
date: 2019-05-27                      
author: luojiaqi                      
header-img: img/post-bg-re-vs-ng2.jpg 
catalog: true                         
tags:                                 
    强化学习                           
---



# 强化学习基本概念

## 机器学习算法大致分为三种
+监督学习（如回归，分类）
+非监督学习（如聚类，降维）
+强 化学习
## 何为强化学习

定义: Reinforcement learning is learning what to do ----how to map situations to actions ---- so as to maximize a numerical reward signal.

强化学习关注的时智能体如何在环境中采取一系列行为，从而获得最大的累计回报。 

通过强化学习，一个智能体影噶知道在什么状态下应该采取什么行为，RL是从环境状态到动作的映射的学习，我们把这个映射称为***策略***。

## 解决些问题
例1.flappy bird，我们让小鸟自行进行游戏，但是我们没有小鸟的动力学模型，也不打算了解它的动力学模型。这时就可以给它设计一个强化学习算法，让小鸟不断地进行游戏。如果小鸟撞到柱子了，就获得**-1**的汇报否则获得**0**汇报。通过若干次这样的训练，最终可以得到一直飞行技能高超的小鸟。它知道咋什么情况下采取什么动作来躲避柱子。

例2.国际象棋机器人。假设要构建一个下国际象棋的机器，这种情况下不能使用监督学习。首先，我们本身不是优秀的棋手，而请象棋老师来遍历每个状态下的最佳棋步的代价过于昂贵。其次，每个棋步的好坏判断不是孤立的，要依赖于对手的选择和局势的变化。是一系列的棋步组成的策略决定了是否能赢得比赛。下棋过程的唯一的反馈实在最后赢得或者输掉棋局时才产生的。这种情况我们可以采用强化学习，通过不断地探索和试错学习，强化学习可以获得某种下棋地策略，并在每个状态下都选择最有可能获胜地棋步。目前这种算法已经在棋类游戏中得到了广泛应用。

## 强化习和监督学习的区别
+ 强化学习是**试错学习**，由于没有直接的指导信息，智能体要不断地与环境进行交互，通过试错的方式来获得最佳策略。
+ **延迟汇报**，强化学习的指导信息很少，往往是在时候（最后一个状态）才给出，这就导致了一个问题，就是获得回报后，如何将汇报分配给前面的状态。



# 马尔可夫决策过程

## 马尔可夫模型的几类子模型

马尔可夫性（无后效性），指系统的下沟状态只与 当前状态信息有关，与更早之前的状态无关。

马尔可夫决策过程（Markov Decision Process，MDP）也具有马尔可夫性，不同在于MDP考虑了动作，即系统的下个状态不仅和当前的状态有关，也和当前采取的动作有关。

|                | 不考虑动作          | 考虑动作 |
| -------------- | ---------------- | ------------------ |
| 状态完全可见   | 马尔科夫链(MC)      | 马尔可夫决策过程(MDP)               |
| 状态不完全可见 | 隐马尔可夫模型(HMM) | 不完全可观察马尔可夫决策过程(POMDP) |

## 马尔可夫决策过程

一个马尔可夫决策过程由一个四元组构成$$M=(S,A,P_{sa},R)$$

+ $S$：表示状态集（states），有$s\in S$,$s_i$表示第$i$步的动作。
+ $A$：表示一组动作（actions），有$a\in A$，$a_i$表示第$i$步的动作。
+ $P_{sa}$：表示状态转移概率，$P_{sa}$表示在当前状态$s\in S$下，经过$a\in A$作用后，会转移到其他状态的概率分布情况。$P_{sa}^{s'} = Pr \left\{ S_{t+1}=s'|S_t=s,A_t=a \right\}$
+ $R$：$S\times A\to R$，$R$是回报函数（reward function）。如果一组$(s,a)$转移到了下一个状态 $s'$，那么回报函数可记为$r(s'|s,a)$。

MDP的动态过程可以表示成下图

![img](https://images0.cnblogs.com/blog/489049/201401/132312526273.jpg)

## 值函数（value function）

强化学习学到的是一个从环境状态到动作的映射（即行为策略），记为策略$\pi:S\to A$。而强化学习往往具有延迟汇报的特点：如果在第$n$步输掉了棋，那么只有状态$s_n$和动作$a_n$获得了立即汇报$r(s_n,a_n)=-1$，前面的所有状态立即汇报均为0。所以对于之前的任意状态$s$和动作$a$，立即回报函数$r(s,a)$无法说明策略的好坏，因此需要定义值函数（value function，又称效用函数）来表明当前状态下策略的长期影响。



用$V^\pi(s)$表示策略$\pi$下，状态$s$的值函数。$r_i$表示未来第$i$步的立即汇报，常见的值函数有以下三种：

a）$V^\pi (s)=E_\pi[\sum\limits^h_{i=0}r_i|s_0=s]$

b）$V^\pi (s)=\lim\limits_{h\to\infty}E_\pi[\frac{1}{h}\sum^h_{i=0}r_i|s_0=s]$

c）$V^\pi (s)=E_\pi[\sum\limits^\infty_{i=0}\gamma^i r_i|s_0=s]$

其中：

a）是采用策略$\pi$的情况下未来有限$h$步的期望立即回报总和；

b）是采用策略$\pi$的情况下期望的平均汇报；

c）是值函数最常见的形式，式中$\gamma \in[0,1]$称为折合因子，表明了未来的回报相对于当前回报的重要程度。特别的，$\gamma=0$时，相当于只考虑立即回报不考虑长期回报；$\gamma=1$时，将长期回报和立即回报看得同等重要。接下来只讨论第三种形式。



将第三种形式展开，其中$r_i$表示未来第$i$步的回报，$s'$表示下一步的状态，则有：

$$V^\pi (s)=E_\pi [r_0+\gamma r_1+\gamma^2r_2+\gamma^3r_3+...|s_0=s]$$

$=E_\pi [r_0+\gamma E[\gamma r_1+\gamma^2 r_2+\gamma^3 r_3+...|s_0=s]]$

$=E_\pi [r(s'|s,a)+\gamma V^\pi(s')|s_0=s ]$

给定策略$\pi$和初始状态$s$，则动作$a=\pi(s)]$，下个时刻将以概率$p(s'|s,a)$变为下个状态$s'$，上式可重写为：

$$V^\pi (s)=\sum\limits_{s'\in S}p(s'|s,a)[r(s'|s,a)+\gamma V^\pi (s')]$$

这种值函数称为**状态值函数（state value function）**，在$V_\pi (s)$中，策略$\pi$和初始状态$s$是我们给定的，而初始动作$a$是由策略$\pi$和状态$s$决定的，即$a=\pi(s)$。

定义**动作值函数（action value function，Q函数）**如下

$$Q^\pi (s,a)=E[\sum\limits^\infty_{i=0}\gamma^ir_i|s_0=s,a_0=a]$$

给的那个当前状态$s$和当前动作$a$，在未来遵循策略$\pi$，那么系统将以概率$p(s'|s,a)$变为下个状态$s'$，上式可重写为：

$$Q^\pi (s,a)=\sum\limits_{s'\in S}p(s'|s,a)[r(s'|s,a)+\gamma V^\pi (s')]$$

在$Q^\pi (s,a)$中，不仅策略$\pi$和出事状态$s$是我们给定的，当前的动作$a$也hi我们给定的，这是$Q^\pi(s,a)$和$V^\pi(s)$的主要区别。